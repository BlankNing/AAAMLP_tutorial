{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec097b04",
   "metadata": {},
   "source": [
    "# Generate new features for time series related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ec92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create a series of datetime with a frequency of 10 hours\n",
    "s = pd.date_range('2020-01-06', '2020-01-10', freq='10H').to_series()\n",
    "# create some features based on datetime\n",
    "features = {\n",
    "    \"dayofweek\": s.dt.dayofweek.values,\n",
    "    \"dayofyear\": s.dt.dayofyear.values,\n",
    "    \"hour\": s.dt.hour.values,\n",
    "    \"is_leap_year\": s.dt.is_leap_year.values,\n",
    "    \"quarter\": s.dt.quarter.values,\n",
    "#     \"weekofyear\": s.dt.weekofyear.values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd2e61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dayofweek': array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3], dtype=int32),\n",
       " 'dayofyear': array([6, 6, 6, 7, 7, 8, 8, 8, 9, 9], dtype=int32),\n",
       " 'hour': array([ 0, 10, 20,  6, 16,  2, 12, 22,  8, 18], dtype=int32),\n",
       " 'is_leap_year': array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]),\n",
       " 'quarter': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ca2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    # create a bunch of features using the date column\n",
    "    df.loc[:, 'year'] = df['date'].dt.year\n",
    "    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n",
    "    df.loc[:, 'month'] = df['date'].dt.month\n",
    "    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n",
    "    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n",
    "\n",
    "    # create an aggregate dictionary\n",
    "    aggs = {}\n",
    "    # for aggregation by month, we calculate the\n",
    "    # number of unique month values and also the mean\n",
    "    aggs['month'] = ['nunique', 'mean']\n",
    "    aggs['weekofyear'] = ['nunique', 'mean']\n",
    "    # we aggregate by num1 and calculate sum, max, min \n",
    "    # and mean values of this column\n",
    "    aggs['num1'] = ['sum','max','min','mean']\n",
    "    # for customer_id, we calculate the total count\n",
    "    aggs['customer_id'] = ['size']\n",
    "    # again for customer_id, we calculate the total unique\n",
    "    aggs['customer_id'] = ['nunique']\n",
    "\n",
    "    # we group by customer_id and calculate the aggregates\n",
    "    agg_df = df.groupby('customer_id').agg(aggs)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dd59f",
   "metadata": {},
   "source": [
    "when you are grouping \n",
    "on a categorical column, you will get features like a list of values which are time \n",
    "distributed. In these cases, you can create a bunch of statistical features such as:\n",
    "- Mean\n",
    "- Max\n",
    "- Min\n",
    "- Unique\n",
    "- Skew\n",
    "- Kurtosis\n",
    "- Kstat\n",
    "- Percentile\n",
    "- Quantile\n",
    "- Peak to peak\n",
    "\n",
    "and many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_dict = {}\n",
    "# calculate mean\n",
    "feature_dict['mean'] = np.mean(x)\n",
    "# calculate max\n",
    "feature_dict['max'] = np.max(x)\n",
    "# calculate min\n",
    "feature_dict['min'] = np.min(x)\n",
    "# calculate standard deviation\n",
    "feature_dict['std'] = np.std(x)\n",
    "# calculate variance\n",
    "feature_dict['var'] = np.var(x)\n",
    "# peak-to-peak\n",
    "feature_dict['ptp'] = np.ptp(x)\n",
    "# percentile features\n",
    "feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "# quantile features\n",
    "feature_dict['quantile_5'] = np.quantile(x, 0.05)\n",
    "feature_dict['quantile_95'] = np.quantile(x, 0.95)\n",
    "feature_dict['quantile_99'] = np.quantile(x, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsfresh is good at handling these type of feature engineering\n",
    "from tsfresh.feature_extraction import feature_calculators as fc\n",
    "# tsfresh based features\n",
    "feature_dict['abs_energy'] = fc.abs_energy(x)\n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x)\n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x)\n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x)\n",
    "feature_dict['mean_change'] = fc.mean_change(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce287d",
   "metadata": {},
   "source": [
    "This is not all; tsfresh offers hundreds of features and tens of variations of different \n",
    "features that you can use for time series (list of values) based features. In the \n",
    "examples above, x is a list of values. But that’s not all. There are many other features \n",
    "that you can create for numerical data with or without categorical data. A simple \n",
    "way to generate many features is just to create a bunch of polynomial features. For \n",
    "example, a second-degree polynomial feature from two features “a” and “b” would \n",
    "include: “a”, “b”, “ab”, “a2\n",
    "” and “b2\n",
    "”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7454a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate a random dataframe with \n",
    "# 2 columns and 100 rows\n",
    "df = pd.DataFrame(\n",
    " np.random.rand(100, 2),\n",
    " columns=[f\"f_{i}\" for i in range(1, 3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53bbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(\n",
    " degree=2,\n",
    " interaction_only=False,\n",
    " include_bias=False\n",
    ")\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(\n",
    " poly_feats,\n",
    " columns=[f\"f_{i}\" for i in range(1, num_feats + 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ee9c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450219</td>\n",
       "      <td>0.473560</td>\n",
       "      <td>0.202697</td>\n",
       "      <td>0.213206</td>\n",
       "      <td>0.224259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.784832</td>\n",
       "      <td>0.973155</td>\n",
       "      <td>0.615961</td>\n",
       "      <td>0.763763</td>\n",
       "      <td>0.947031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.864824</td>\n",
       "      <td>0.251149</td>\n",
       "      <td>0.747921</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.063076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.855703</td>\n",
       "      <td>0.401482</td>\n",
       "      <td>0.732228</td>\n",
       "      <td>0.343549</td>\n",
       "      <td>0.161188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.683066</td>\n",
       "      <td>0.644092</td>\n",
       "      <td>0.466579</td>\n",
       "      <td>0.439957</td>\n",
       "      <td>0.414855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.504936</td>\n",
       "      <td>0.792664</td>\n",
       "      <td>0.254961</td>\n",
       "      <td>0.400245</td>\n",
       "      <td>0.628316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.674998</td>\n",
       "      <td>0.963180</td>\n",
       "      <td>0.455623</td>\n",
       "      <td>0.650145</td>\n",
       "      <td>0.927716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.296746</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.088058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.630630</td>\n",
       "      <td>0.453290</td>\n",
       "      <td>0.397694</td>\n",
       "      <td>0.285858</td>\n",
       "      <td>0.205472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.455930</td>\n",
       "      <td>0.989323</td>\n",
       "      <td>0.207872</td>\n",
       "      <td>0.451062</td>\n",
       "      <td>0.978760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f_1       f_2       f_3       f_4       f_5\n",
       "0   0.450219  0.473560  0.202697  0.213206  0.224259\n",
       "1   0.784832  0.973155  0.615961  0.763763  0.947031\n",
       "2   0.864824  0.251149  0.747921  0.217200  0.063076\n",
       "3   0.855703  0.401482  0.732228  0.343549  0.161188\n",
       "4   0.683066  0.644092  0.466579  0.439957  0.414855\n",
       "..       ...       ...       ...       ...       ...\n",
       "95  0.504936  0.792664  0.254961  0.400245  0.628316\n",
       "96  0.674998  0.963180  0.455623  0.650145  0.927716\n",
       "97  0.018934  0.296746  0.000358  0.005619  0.088058\n",
       "98  0.630630  0.453290  0.397694  0.285858  0.205472\n",
       "99  0.455930  0.989323  0.207872  0.451062  0.978760\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a05cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ae6dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09333398704768911"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.f_3.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fcaf20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04661263999992299"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.f_3.apply(lambda x: np.log(1 + x)).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834e520",
   "metadata": {},
   "source": [
    "you can see the variance of the feature column is actually getting lower.\n",
    "\n",
    "Sometimes, instead of log, you can also take exponential. A very interesting case is \n",
    "when you use a log-based evaluation metric, for example, RMSLE. In that case, \n",
    "you can train on log-transformed targets and convert back to original using \n",
    "exponential on the prediction. That would help optimize the model for the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb66e6",
   "metadata": {},
   "source": [
    "# Missing Value/Nan feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309a62b",
   "metadata": {},
   "source": [
    "For categorical features, let’s keep it super simple. If you ever encounter missing \n",
    "values in categorical features, treat is as a new category! As simple as this is, it \n",
    "(almost) always works!\n",
    "\n",
    "\n",
    "One way to fill missing values in numerical data would be to choose a value that \n",
    "does not appear in the specific feature and fill using that. For example, let’s say 0 \n",
    "is not seen in the feature. So, we fill all the missing values using 0. This is one of \n",
    "the ways but might not be the most effective. One of the methods that works better \n",
    "than filling 0s for numerical data is to fill with mean instead. You can also try to fill \n",
    "with the median of all the values for that feature, or you can use the most common \n",
    "value to fill the missing values. There are just so many ways to do this.\n",
    "\n",
    "\n",
    "A fancy way of filling in the missing values would be to use a k-nearest neighbour\n",
    "method. You can select a sample with missing values and find the nearest \n",
    "neighbours utilising some kind of distance metric, for example, Euclidean distance. \n",
    "Then you can take the mean of all nearest neighbours and fill up the missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de54e93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11. ,  2. , 12. ,  3.5,  7. , 14. ],\n",
       "       [14. ,  5.5,  8. ,  1. ,  5. , 13. ],\n",
       "       [ 4. , 13. , 13. , 10. ,  3. ,  3. ],\n",
       "       [ 5. ,  8. ,  6. , 10. ,  2. , 13. ],\n",
       "       [ 9. ,  9. , 10. ,  7. ,  6.5, 11. ],\n",
       "       [10. ,  9. , 10. ,  6. , 11. , 14. ],\n",
       "       [ 2. ,  9. ,  4. ,  8. ,  9. ,  8. ],\n",
       "       [ 3.5, 13. ,  2. ,  7. ,  5.5, 10. ],\n",
       "       [ 2. ,  5. ,  8.5, 13. ,  5.5, 10.5],\n",
       "       [ 7. , 11. , 13. ,  9. ,  6. ,  4. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import impute\n",
    "# create a random numpy array with 10 samples\n",
    "# and 6 features and values ranging from 1 to 15\n",
    "X = np.random.randint(1, 15, (10, 6))\n",
    "\n",
    "# convert the array to float\n",
    "X = X.astype(float)\n",
    "# randomly assign 10 elements to NaN (missing)\n",
    "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
    "# use 2 nearest neighbours to fill na values\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=2)\n",
    "knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b46a79",
   "metadata": {},
   "source": [
    "Another way of imputing missing values in a column would be to train a regression \n",
    "model that tries to predict missing values in a column based on other columns. So, \n",
    "you start with one column that has a missing value and treat this column as the \n",
    "target column for regression model without the missing values. Using all the other \n",
    "columns, you now train a model on samples for which there is no missing value in \n",
    "the concerned column and then try to predict target (the same column) for the \n",
    "samples that were removed earlier. This way, you have a more robust model based \n",
    "imputation.\n",
    "\n",
    "Always remember that imputing values for tree-based models is unnecessary as they \n",
    "can handle it themselves.\n",
    "\n",
    "You're absolutely right to emphasize this point. Tree-based models indeed have a unique advantage when it comes to handling missing values, making imputation often unnecessary. Let's explore why this is the case:\n",
    "Why Tree-Based Models Can Handle Missing Values\n",
    "1. Splitting Mechanism\n",
    "Tree-based models, such as decision trees, random forests, and gradient boosting machines, work by making sequential splits in the data based on feature values. When encountering missing values, these models have built-in methods to handle them:\n",
    "They can treat \"missing\" as a separate category\n",
    "They can use surrogate splits based on other correlated features\n",
    "2. Implicit Imputation\n",
    "Tree-based models perform a form of implicit imputation during the training process:\n",
    "For each split, the algorithm determines the best way to handle missing values to minimize the loss function\n",
    "This approach is often more effective than pre-processing imputation, as it considers the specific context of each split\n",
    "3. Preservation of Information\n",
    "By not imputing values beforehand:\n",
    "The model preserves the information that a value was missing\n",
    "It can potentially discover patterns related to missingness itself\n",
    "4. Robustness to Different Types of Missing Data\n",
    "Tree-based models can effectively handle different types of missing data:\n",
    "Missing Completely at Random (MCAR)\n",
    "Missing at Random (MAR)\n",
    "Missing Not at Random (MNAR)\n",
    "Advantages Over Explicit Imputation\n",
    "Saves preprocessing time and effort\n",
    "Avoids introducing bias through imputation\n",
    "Maintains the original data structure\n",
    "Can capture complex relationships between missing values and other features\n",
    "\n",
    "Exceptions and Considerations\n",
    "\n",
    "While tree-based models generally handle missing values well, there are some scenarios where imputation might still be considered:\n",
    "\n",
    "When using tree-based models for feature importance analysis\n",
    "In ensemble methods that include non-tree-based models\n",
    "When dealing with extremely high proportions of missing data\n",
    "\n",
    "In conclusion, the ability of tree-based models to handle missing values internally is a significant advantage, often making explicit imputation unnecessary and potentially counterproductive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a69e6",
   "metadata": {},
   "source": [
    "And always remember to scale or normalize your \n",
    "features if you are using linear models like logistic regression or a model like SVM. \n",
    "Tree-based models will always work fine without any normalization of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10c535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
