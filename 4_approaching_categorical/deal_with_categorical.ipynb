{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6423f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "# Can be used in tree-based models: XGBoost,GBM,LightGBM\n",
    "# Cannot be used in linear models, svm or NN where the data are\n",
    "# expected to be normalised (standardized)\n",
    "mapping = {\n",
    " \"Freezing\": 0,\n",
    " \"Warm\": 1,\n",
    " \"Cold\": 2,\n",
    " \"Boiling Hot\": 3,\n",
    " \"Hot\": 4,\n",
    " \"Lava Hot\": 5 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0865def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./test.csv\")\n",
    "df.loc[:, \"ord_2\"] = df.ord_2.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aff2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "0.0    95287\n",
       "1.0    82940\n",
       "2.0    65042\n",
       "3.0    56624\n",
       "4.0    44509\n",
       "5.0    43493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cdb607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do so\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read the data\n",
    "df = pd.read_csv(\"./test.csv\")\n",
    "# fill NaN values in ord_2 column\n",
    "df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\")\n",
    "# initialize LabelEncoder\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "# fit label encoder and transform values on ord_2 column\n",
    "# P.S: do not use this directly. fit first, then transform\n",
    "df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f276061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "2    95287\n",
       "6    82940\n",
       "1    65042\n",
       "0    56624\n",
       "3    44509\n",
       "4    43493\n",
       "5    12105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba41a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# to fit this for svm/neural network, we can binarize the label\n",
    "# by splitting them into n features 2^n\n",
    "\n",
    "# There are normal binary data storation and sparse version\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# create our example feature matrix\n",
    "example = np.array(\n",
    " [\n",
    " [0, 0, 1],\n",
    " [1, 0, 0],\n",
    " [1, 0, 1]\n",
    " ]\n",
    ")\n",
    "# print size of the origin matrix\n",
    "print(example.nbytes)\n",
    "# convert numpy array to sparse CSR matrix\n",
    "# record the position of 1s\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(sparse_example.data.nbytes)\n",
    "# total storage space for sparse matrix\n",
    "print(\n",
    " sparse_example.data.nbytes + \n",
    " sparse_example.indptr.nbytes + \n",
    " sparse_example.indices.nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c49b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 144\n",
      "Size of sparse array: 24\n",
      "Full size of sparse array: 52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# create binary matrix\n",
    "example = np.array(\n",
    " [\n",
    " [0, 0, 0, 0, 1, 0],\n",
    " [0, 1, 0, 0, 0, 0],\n",
    " [1, 0, 0, 0, 0, 0]\n",
    " ]\n",
    ")\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\")\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "full_size = (\n",
    " sparse_example.data.nbytes + \n",
    " sparse_example.indptr.nbytes + \n",
    " sparse_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a598aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56624, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./test.csv\")\n",
    "df[df.ord_2 == \"Boiling Hot\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "072234a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Boiling Hot    56624\n",
       "Cold           65042\n",
       "Freezing       95287\n",
       "Hot            44509\n",
       "Lava Hot       43493\n",
       "Warm           82940\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90830274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         56624.0\n",
       "1         65042.0\n",
       "2         82940.0\n",
       "3         44509.0\n",
       "4         43493.0\n",
       "           ...   \n",
       "399995    82940.0\n",
       "399996        NaN\n",
       "399997    56624.0\n",
       "399998    43493.0\n",
       "399999    95287.0\n",
       "Name: id, Length: 400000, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"ord_2\"])[\"id\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c04dd0",
   "metadata": {},
   "source": [
    "# Deal with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f819b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Freezing       95287\n",
       "Warm           82940\n",
       "Cold           65042\n",
       "Boiling Hot    56624\n",
       "Hot            44509\n",
       "Lava Hot       43493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./test.csv\")\n",
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4856d60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Freezing       95287\n",
       "Warm           82940\n",
       "Cold           65042\n",
       "Boiling Hot    56624\n",
       "Hot            44509\n",
       "Lava Hot       43493\n",
       "NONE           12105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12398c",
   "metadata": {},
   "source": [
    "# Rare Category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dafa1174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_4\n",
       "N       26725\n",
       "P       25391\n",
       "Y       24871\n",
       "A       24478\n",
       "R       22053\n",
       "U       22017\n",
       "M       21487\n",
       "X       21419\n",
       "C       21133\n",
       "H       20501\n",
       "Q       19963\n",
       "T       19765\n",
       "O       17347\n",
       "B       16705\n",
       "E       14654\n",
       "K       14322\n",
       "I       13123\n",
       "NONE    11933\n",
       "D       11401\n",
       "F       11154\n",
       "W        5634\n",
       "Z        3973\n",
       "S        3197\n",
       "G        2290\n",
       "V        2042\n",
       "J        1331\n",
       "L        1091\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c36faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We say that wherever the value count for a certain category is less than 2000, \n",
    "replace it with rare. So, now, when it comes to test data, all the new, unseen \n",
    "categories will be mapped to “RARE”, and all missing values will be mapped to \n",
    "“NONE”.\n",
    "This approach will also ensure that the model works in a live setting, even if you \n",
    "have new categories.\n",
    "'''\n",
    "\n",
    "df.ord_4 = df.ord_4.fillna(\"NONE\")\n",
    "df.loc[df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000, \"ord_4\"] = \"RARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca49e2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_4\n",
       "N       26725\n",
       "P       25391\n",
       "Y       24871\n",
       "A       24478\n",
       "R       22053\n",
       "U       22017\n",
       "M       21487\n",
       "X       21419\n",
       "C       21133\n",
       "H       20501\n",
       "Q       19963\n",
       "T       19765\n",
       "O       17347\n",
       "B       16705\n",
       "E       14654\n",
       "K       14322\n",
       "I       13123\n",
       "NONE    11933\n",
       "D       11401\n",
       "F       11154\n",
       "W        5634\n",
       "Z        3973\n",
       "S        3197\n",
       "RARE     2422\n",
       "G        2290\n",
       "V        2042\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24367f",
   "metadata": {},
   "source": [
    "# Categorical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8779c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read training data\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "#read test data\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "# create a fake target column for test data\n",
    "# since this column doesn't exist\n",
    "test.loc[:, \"target\"] = -1\n",
    "# concatenate both training and test data\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "# make a list of features we are interested in\n",
    "# id and target is something we should not encode\n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
    "\n",
    "# loop over the features list\n",
    "for feat in features:\n",
    "    # create a new instance of LabelEncoder for each feature\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    # note the trick here\n",
    "    # since its categorical data, we fillna with a string\n",
    "    # and we convert all the data to string type\n",
    "    # so, no matter its int or float, its converted to string\n",
    "    # int/float but categorical!!!\n",
    "    temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
    "    # we can use fit_transform here as we do not\n",
    "    # have any extra test data that we need to\n",
    "    # transform on separately\n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    " \n",
    "# split the training and test data again \n",
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "\n",
    "\n",
    "test = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44991e73",
   "metadata": {},
   "source": [
    "Before going to any kind of model building, it’s essential to take care of cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78939e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py\n",
    "# import pandas and model_selection module of scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "if __name__ == \"__main__\":\n",
    "    # Read training data\n",
    "    df = pd.read_csv(\"./train.csv\")\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "\n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # fetch labels\n",
    "    y = df.target.values\n",
    "\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"./cat_train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1515cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./cat_train_folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28d10101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfold\n",
       "0    120000\n",
       "1    120000\n",
       "2    120000\n",
       "3    120000\n",
       "4    120000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.kfold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46f88658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    97536\n",
       "1    22464\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==0].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783caea",
   "metadata": {},
   "source": [
    "Build simple logistic regression model with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "945c9468",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:39\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.fit(x_train, df_train.target.values)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesn’t matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "\n",
    "        # get training data using folds\n",
    "        df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "        # get validation data using folds\n",
    "        df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "        # initialize OneHotEncoder from scikit-learn\n",
    "        ohe = preprocessing.OneHotEncoder()\n",
    "        # fit ohe on training + validation features\n",
    "        full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]],\n",
    "        axis=0\n",
    "        )\n",
    "        ohe.fit(full_data[features])\n",
    "        # transform training data\n",
    "        x_train = ohe.transform(df_train[features])\n",
    "        # transform validation data\n",
    "        x_valid = ohe.transform(df_valid[features])\n",
    "        \n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(auc)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # run function for fold = 0\n",
    "    # we can just replace this number and \n",
    "    # run this for any fold\n",
    "    run(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
